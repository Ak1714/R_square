{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage with project summary LCDA_Part1.md # Analyzing and vizualizing the data LCDA_Part2.md # Clustering & PCA LCDA_Part3.md # Building models Data source https://www.kaggle.com/wordsforthewise/lending-club Summary Lending Club is the world\u2019s leading online marketplace for connecting borrowers and investors. In this project the goal is to build a robust model using logistic regression and classification tree for predicting the final status of a loan based on variables available at the time when the loan is granted. Vetting data Reading the lendingdataclub 2017 Q3 csv file and scanning the file for all columns by skipping the first line since it is not required. colNames <- scan(\"LoanStats2017Q3.csv\", what=\"character\", skip=1, nlines=1, sep=\",\") Specifying the character and factor columns for categorical data, since rest of it is mostly numerical and date data. characterColumns <- c(\"id\", \"member_id\", \"emp_title\", \"issue_d\", \"url\", \"desc\", \"zip_code\", \"addr_state\", \"earliest_cr_line\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\", \"sec_app_earliest_cr_line\", \"hardship_type\", \"hardship_reason\", \"hardship_start_date\", \"hardship_end_date\", \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\") factorColumns <- c(\"term\", \"grade\", \"sub_grade\", \"emp_length\", \"home_ownership\", \"verification_status\", \"loan_status\", \"pymnt_plan\", \"purpose\", \"title\", \"initial_list_status\", \"policy_code\", \"application_type\", \"verification_status_joint\", \"hardship_flag\", \"hardship_status\", \"hardship_loan_status\", \"disbursement_method\", \"debt_settlement_flag\", \"settlement_status\") Excluding the top two rows and any other row that is not part of the main data by specifying the exact number of rows (122701) lendingRows <- 122701 lendingData <- read.table(\"LoanStats2017Q3.csv\", skip=1, sep=\",\", nrows=lendingRows, colClasses = myColClasses, header=TRUE) After reading the columns, two columns: revol_util and int_rate have % signs. We don\u2019t want R to interpret it as character strings. Hence, we can convert to character strings and getting rid of the \u2018%\u2019 and then back to numeric. lendingData$revol_util <- as.character(lendingData$revol_util) lendingData$revol_util <- sub(\"%\", \"\", lendingData$revol_util) lendingData$revol_util <- as.numeric(lendingData$revol_util) lendingData$int_rate <- as.character(lendingData$int_rate) lendingData$int_rate <- sub(\"%\", \"\", lendingData$int_rate) lendingData$int_rate <- as.numeric(lendingData$int_rate) Formatting the columns for date data and specifying the first of the month since the data only has month and year. dateColumns <- c(\"issue_d\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\", \"sec_app_earliest_cr_line\", \"hardship_start_date\", \"hardship_end_date\", \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\", \"earliest_cr_line\") Using function strptime(x, format), where x is a character vector of dates and format is a character string of the dates, using percent symbols with characters to specify what types of date and time information.%d: Day of the month as decimal number (01--31)%b: Abbreviated month name in the current locale on this platform%Y: Year with century e.g.: 2015 lendingData[,dateColumns] <- apply(lendingData[,dateColumns], 2, function(x) { strptime(paste(\"1\", x), \"%d %b-%Y\")}) lendingData[,dateColumns] <- apply(lendingData[,dateColumns], 2, function(x) { strptime(paste(\"1\", x), \"%d %b-%Y\")}) Saving the file as R object for faster loading. Then counting the number of each loan status. save(lendingData, file=\"LoanStats2017Q3.rda\") load(\"LoanStats2017Q3.rda\") (table(lendingData$loan_status)) # Charged Off Current Default Fully Paid In Grace Period # 12876 65816 161 40471 752 # Late (16-30 days) Late (31-120 days) # 503 2122","title":"Home"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage with project summary LCDA_Part1.md # Analyzing and vizualizing the data LCDA_Part2.md # Clustering & PCA LCDA_Part3.md # Building models","title":"Project layout"},{"location":"#data-source","text":"https://www.kaggle.com/wordsforthewise/lending-club","title":"Data source"},{"location":"#summary","text":"Lending Club is the world\u2019s leading online marketplace for connecting borrowers and investors. In this project the goal is to build a robust model using logistic regression and classification tree for predicting the final status of a loan based on variables available at the time when the loan is granted.","title":"Summary"},{"location":"#vetting-data","text":"Reading the lendingdataclub 2017 Q3 csv file and scanning the file for all columns by skipping the first line since it is not required. colNames <- scan(\"LoanStats2017Q3.csv\", what=\"character\", skip=1, nlines=1, sep=\",\") Specifying the character and factor columns for categorical data, since rest of it is mostly numerical and date data. characterColumns <- c(\"id\", \"member_id\", \"emp_title\", \"issue_d\", \"url\", \"desc\", \"zip_code\", \"addr_state\", \"earliest_cr_line\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\", \"sec_app_earliest_cr_line\", \"hardship_type\", \"hardship_reason\", \"hardship_start_date\", \"hardship_end_date\", \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\") factorColumns <- c(\"term\", \"grade\", \"sub_grade\", \"emp_length\", \"home_ownership\", \"verification_status\", \"loan_status\", \"pymnt_plan\", \"purpose\", \"title\", \"initial_list_status\", \"policy_code\", \"application_type\", \"verification_status_joint\", \"hardship_flag\", \"hardship_status\", \"hardship_loan_status\", \"disbursement_method\", \"debt_settlement_flag\", \"settlement_status\") Excluding the top two rows and any other row that is not part of the main data by specifying the exact number of rows (122701) lendingRows <- 122701 lendingData <- read.table(\"LoanStats2017Q3.csv\", skip=1, sep=\",\", nrows=lendingRows, colClasses = myColClasses, header=TRUE) After reading the columns, two columns: revol_util and int_rate have % signs. We don\u2019t want R to interpret it as character strings. Hence, we can convert to character strings and getting rid of the \u2018%\u2019 and then back to numeric. lendingData$revol_util <- as.character(lendingData$revol_util) lendingData$revol_util <- sub(\"%\", \"\", lendingData$revol_util) lendingData$revol_util <- as.numeric(lendingData$revol_util) lendingData$int_rate <- as.character(lendingData$int_rate) lendingData$int_rate <- sub(\"%\", \"\", lendingData$int_rate) lendingData$int_rate <- as.numeric(lendingData$int_rate) Formatting the columns for date data and specifying the first of the month since the data only has month and year. dateColumns <- c(\"issue_d\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\", \"sec_app_earliest_cr_line\", \"hardship_start_date\", \"hardship_end_date\", \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\", \"earliest_cr_line\") Using function strptime(x, format), where x is a character vector of dates and format is a character string of the dates, using percent symbols with characters to specify what types of date and time information.%d: Day of the month as decimal number (01--31)%b: Abbreviated month name in the current locale on this platform%Y: Year with century e.g.: 2015 lendingData[,dateColumns] <- apply(lendingData[,dateColumns], 2, function(x) { strptime(paste(\"1\", x), \"%d %b-%Y\")}) lendingData[,dateColumns] <- apply(lendingData[,dateColumns], 2, function(x) { strptime(paste(\"1\", x), \"%d %b-%Y\")}) Saving the file as R object for faster loading. Then counting the number of each loan status. save(lendingData, file=\"LoanStats2017Q3.rda\") load(\"LoanStats2017Q3.rda\") (table(lendingData$loan_status)) # Charged Off Current Default Fully Paid In Grace Period # 12876 65816 161 40471 752 # Late (16-30 days) Late (31-120 days) # 503 2122","title":"Vetting data"},{"location":"LCDA_Part1/","text":"Exploratory analysis and visualizing to get a feel of the distribution patterns Importing packages library(ggplot2) library(dplyr) Exploratory analysis Determining the mean and median of the borrowers' months since last public record (mths_since_last_record). Finding the number of observations for which this value is missing and creating a new variable where the value is imputed with the median. load(\"LoanStats2017Q3.rda\") mean(lendingData$mths_since_last_record) ## [1] NA Counting number of nulls: sum(is.na(lendingData$mths_since_last_record)) ## [1] 103863 Calculating mean without NULL values: mean(lendingData$mths_since_last_record, na.rm = TRUE) ## [1] 75.79589 Calculating the median without null values: median(lendingData$mths_since_last_record, na.rm = TRUE) ## [1] 79 Creating a new column for imputing median in place of NULL. Then imputing median in the new column in place of NULLs: medianCol <- c(lendingData$mths_since_last_record) medianCol[is.na(medianCol)] <- median(medianCol, na.rm = TRUE) sum(is.na(medianCol)) ## [1] 0 Calculating mean & median for the new column: mean(medianCol) ## [1] 78.50808 median(medianCol) ## [1] 79 Histogram We plot a histogram for the settlement amount for loans using the function hist(); hist(lendingData$settlement_amount) The above histogram looks to be right skewed. Also, the column has a large range between $331.44 to $24102, it makes sense to do a log transformation to make it closer to a normal distribution. The below histogram appears to be close to a normal distribution. logSettleamt <- log(lendingData$settlement_amount) hist(logSettleamt, labels=TRUE) Boxplot Creating a boxplot of interest rate (int_rate) for each loan status (loan_status) to find the status that has loans with the highest median interest rate: boxplot(lendingData$int_rate ~ lendingData$loan_status, col = 6, labels = TRUE, las = 2, par(mar=c(8,5,2,2))) The median values for all loan statuses are summarized below using dplyr and group_by: lendingData %>% dplyr::select(loan_status, int_rate) %>% group_by(loan_status) %>% summarize(medianVal = median(int_rate, na.rm = TRUE)) %>% arrange(loan_status) ## # A tibble: 7 x 2 ## loan_status medianVal ## <fct> <dbl> ## 1 Charged Off 15.0 ## 2 Current 12.0 ## 3 Default 15.0 ## 4 Fully Paid 12.6 ## 5 In Grace Period 14.1 ## 6 Late (16-30 days) 14.1 ## 7 Late (31-120 days) 15.0 From the boxplot, loan statuses: Charged Off, Default and Late (31-120 days) tend to have the highest median interest rates since these borrowers haven\u2019t been able to pay their interests on time. The interest rates for Fully Paid loans are lower than those of Charged Off ones.","title":"Vizualizing Data"},{"location":"LCDA_Part1/#importing-packages","text":"library(ggplot2) library(dplyr)","title":"Importing packages"},{"location":"LCDA_Part1/#exploratory-analysis","text":"Determining the mean and median of the borrowers' months since last public record (mths_since_last_record). Finding the number of observations for which this value is missing and creating a new variable where the value is imputed with the median. load(\"LoanStats2017Q3.rda\") mean(lendingData$mths_since_last_record) ## [1] NA Counting number of nulls: sum(is.na(lendingData$mths_since_last_record)) ## [1] 103863 Calculating mean without NULL values: mean(lendingData$mths_since_last_record, na.rm = TRUE) ## [1] 75.79589 Calculating the median without null values: median(lendingData$mths_since_last_record, na.rm = TRUE) ## [1] 79 Creating a new column for imputing median in place of NULL. Then imputing median in the new column in place of NULLs: medianCol <- c(lendingData$mths_since_last_record) medianCol[is.na(medianCol)] <- median(medianCol, na.rm = TRUE) sum(is.na(medianCol)) ## [1] 0 Calculating mean & median for the new column: mean(medianCol) ## [1] 78.50808 median(medianCol) ## [1] 79","title":"Exploratory analysis"},{"location":"LCDA_Part1/#histogram","text":"We plot a histogram for the settlement amount for loans using the function hist(); hist(lendingData$settlement_amount) The above histogram looks to be right skewed. Also, the column has a large range between $331.44 to $24102, it makes sense to do a log transformation to make it closer to a normal distribution. The below histogram appears to be close to a normal distribution. logSettleamt <- log(lendingData$settlement_amount) hist(logSettleamt, labels=TRUE)","title":"Histogram"},{"location":"LCDA_Part1/#boxplot","text":"Creating a boxplot of interest rate (int_rate) for each loan status (loan_status) to find the status that has loans with the highest median interest rate: boxplot(lendingData$int_rate ~ lendingData$loan_status, col = 6, labels = TRUE, las = 2, par(mar=c(8,5,2,2))) The median values for all loan statuses are summarized below using dplyr and group_by: lendingData %>% dplyr::select(loan_status, int_rate) %>% group_by(loan_status) %>% summarize(medianVal = median(int_rate, na.rm = TRUE)) %>% arrange(loan_status) ## # A tibble: 7 x 2 ## loan_status medianVal ## <fct> <dbl> ## 1 Charged Off 15.0 ## 2 Current 12.0 ## 3 Default 15.0 ## 4 Fully Paid 12.6 ## 5 In Grace Period 14.1 ## 6 Late (16-30 days) 14.1 ## 7 Late (31-120 days) 15.0 From the boxplot, loan statuses: Charged Off, Default and Late (31-120 days) tend to have the highest median interest rates since these borrowers haven\u2019t been able to pay their interests on time. The interest rates for Fully Paid loans are lower than those of Charged Off ones.","title":"Boxplot"},{"location":"LCDA_Part2/","text":"Cluster Analysis using PCA Importing packages library(dplyr) library(dummies) # dummy variables library(cluster) # Clustering library(rgl) # 3d plot load(\"LoanStats2017Q3.rda\") set.seed(12345) Preparation for Clustering Storing variables in vectors: annualInc <- lendingData$annual_inc loan_amnt <- lendingData$loan_amnt empLength <- lendingData$emp_length homeOwnership <- lendingData$home_ownership dti <- lendingData$dti grade <- lendingData$grade Removing the value 'n/a' from emp_length column and replacing it with NA: empLength[empLength == 'n/a'] <- NA empLength <- addNA(empLength) Storing all variables in a data frame: tempDF <- data.frame(annualInc, loan_amnt, dti, homeOwnership, empLength) Checking for NULL values in the data frame and then imputing them with the median: apply(is.na(tempDF),2,sum) ## annualInc loan_amnt dti homeOwnership empLength ## 0 0 184 0 0 tempDF$dti[is.na(tempDF$dti)] <- median(tempDF$dti, na.rm=TRUE) head(tempDF) ## annualInc loan_amnt dti homeOwnership empLength ## 1 42000 12000 27.74 OWN 10+ years ## 2 79077 16000 15.94 RENT 5 years ## 3 107000 33000 19.06 MORTGAGE < 1 year ## 4 155000 32000 12.35 MORTGAGE 10+ years ## 5 120000 40000 31.11 MORTGAGE 9 years ## 6 32000 7000 12.27 RENT 10+ years Creating a dummy data frame with categorical columns: lendDF <- dummy.data.frame(tempDF, names=c(\"homeOwnership\", \"empLength\")) head(lendDF) ## annualInc loan_amnt dti homeOwnershipANY homeOwnershipMORTGAGE ## 1 42000 12000 27.74 0 0 ## 2 79077 16000 15.94 0 0 ## 3 107000 33000 19.06 0 1 ## 4 155000 32000 12.35 0 1 ## 5 120000 40000 31.11 0 1 ## 6 32000 7000 12.27 0 0 ## homeOwnershipNONE homeOwnershipOWN homeOwnershipRENT empLength< 1 year ## 1 0 1 0 0 ## 2 0 0 1 0 ## 3 0 0 0 1 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 1 0 ## empLength1 year empLength10+ years empLength2 years empLength3 years ## 1 0 1 0 0 ## 2 0 0 0 0 ## 3 0 0 0 0 ## 4 0 1 0 0 ## 5 0 0 0 0 ## 6 0 1 0 0 ## empLength4 years empLength5 years empLength6 years empLength7 years ## 1 0 0 0 0 ## 2 0 1 0 0 ## 3 0 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## empLength8 years empLength9 years empLengthNA ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 1 0 ## 6 0 0 0 Scaling the values in the data frame for clustering: lendDF <- scale(lendDF, center=TRUE, scale=TRUE) head(lendDF) ## annualInc loan_amnt dti homeOwnershipANY homeOwnershipMORTGAGE ## 1 -0.1149444060 -0.2702472 0.39399388 -0.008074829 -0.9699266 ## 2 0.0002058191 0.1458219 -0.18034054 -0.008074829 -0.9699266 ## 3 0.0869264204 1.9141156 -0.02848262 -0.008074829 1.0309975 ## 4 0.2360002614 1.8100983 -0.35507448 -0.008074829 1.0309975 ## 5 0.1273005856 2.6422365 0.55801989 -0.008074829 1.0309975 ## 6 -0.1460014562 -0.7903336 -0.35896827 -0.008074829 -0.9699266 PCA Performing K-means clustering: lendKmeans <- kmeans(lendDF, centers=7) lendPCA <- prcomp(lendDF, retx=TRUE) plot(lendPCA$x[,1:2], col=lendKmeans$cluster, pch=lendKmeans$cluster) legend(\"topright\", title = ,legend = 1:7, col = 1:7, pch = 1:7) Rotation matrix for first 2 Principal components: lendPCA$rotation[,1:2] ## PC1 PC2 ## annualInc 0.061042681 -0.016995412 ## loan_amnt 0.269090193 0.179204905 ## dti 0.063038509 0.195250523 ## homeOwnershipANY -0.003757898 0.005487485 ## homeOwnershipMORTGAGE 0.634478287 0.247075768 ## homeOwnershipNONE -0.001164800 -0.001968247 ## homeOwnershipOWN -0.046104282 -0.423947441 ## homeOwnershipRENT -0.617114633 0.023901256 ## empLength< 1 year -0.002269023 0.458873341 ## empLength1 year -0.130646715 0.040961336 ## empLength10+ years 0.287971605 -0.647021527 ## empLength2 years -0.128632358 0.075200216 ## empLength3 years -0.098845568 0.076617349 ## empLength4 years -0.069703845 0.074853769 ## empLength5 years -0.044876646 0.086490324 ## empLength6 years -0.017936943 0.103554449 ## empLength7 years -0.011065154 0.087224445 ## empLength8 years -0.003133917 0.074989097 ## empLength9 years 0.009005405 0.092448604 ## empLengthNA -0.025108522 -0.010037250 summary(lendPCA) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 1.40804 1.1428 1.12931 1.06944 1.04506 1.04341 1.03802 ## Proportion of Variance 0.09913 0.0653 0.06377 0.05719 0.05461 0.05444 0.05387 ## Cumulative Proportion 0.09913 0.1644 0.22820 0.28538 0.33999 0.39442 0.44830 PCA description After inspecting the first two PCs the observations are as follows: \u2022 The variation on the first component is due to several related variables concerning the borrowers\u2019 loan amount, homes that are mortgaged and borrowers employed for more than 10 years. Borrowers with larger values on the first component will have larger values for these variables. \u2022 The variation on the second component is due to the borrowers employed for less than a year and for 6 years, homes that are mortgaged, and debt-to-income ratio. Borrowers with larger values on the second component will have larger values for these variables. \u2022 The black cluster has negative values for PC1, indicating fewer rented homes and lesser number of borrowers employed between 1 to 4 years inclusive. The scores on PC2 are negative as well, indicating borrowers with lower annual income. \u2022 The pink cluster has positive values for PC1 than the black cluster , indicating a greater number of borrowers whose homes are mortgaged, have higher loan amounts and have been employed for more than 10 years. The scores on PC2 are near zero, indicating that the values are moderate compared to the rest of the data. \u2022 The yellow clusters & cyan clusters overlap with the black cluster and the red cluster & green cluster clusters overlap with pink cluster . This indicates that the overlapping clusters have similar characteristics. \u2022 The blue cluster has the largest positive values for PC2 corresponding to larger number of borrowers with employment term < 1 year, higher mortgaged homes, higher debt-to-income ratio and higher annual income. \u2022 In PC2, there is a trade-off between more borrowers with mortgaged homes and lesser owned homes. Another trade-off between more borrowers with lower employment length (< 1 year) and lower number of borrowers employed for 10+ years. \u2022 In PC1, there is a trade-off between mortgaged homes and lower number of borrowers with rented homes. Another trade-off between more borrowers employed (>10 years) and less borrowers employed from 1 to 4 years. \u2022 The outlier seems to affect strongly the first principal component. Outlier detection #lendPCA$x[,1] which(lendPCA$x[,1] > 20) ## 72164 ## 72164 lendPCA$x[72164, 1] ## [1] 20.42088 Running Kmeans again and replotting: lendDF <- lendDF[-72164,] lendKmeans <- kmeans(lendDF, centers=7) lendPCA <- prcomp(lendDF, retx=TRUE) plot(lendPCA$x[,1:2], col=lendKmeans$cluster, pch=lendKmeans$cluster) legend(\"topright\", title = ,legend = 1:7, col = 1:7, pch = 1:7) plot3d(lendPCA$x[,1:3], col=lendKmeans$cluster, pch=lendKmeans$cluster) lendPCA$rotation[,1:3] Below is the PCA rotation matrix for first 3 PCs. \u2022 PC3 is associated with borrowers\u2019 loan amount, rented homes and employment length more than 10 years. \u2022 The positive variation in the third PC has to do with borrowers with higher rented homes, loan amount and number of borrowers with over 10years of employment. \u2022 The negative variation in the third PC has to do with lower debt-to-income ratio and lesser number of borrowers with own homes. ## PC1 PC2 PC3 ## annualInc 0.031617716 -0.0008315871 0.028532744 ## loan_amnt 0.267672066 0.1832264396 0.115086885 ## dti 0.064235286 0.1896985757 -0.249456961 ## homeOwnershipANY -0.003734964 0.0049164790 -0.033920042 ## homeOwnershipMORTGAGE 0.635696955 0.2469606171 0.069240871 ## homeOwnershipNONE -0.001156300 -0.0023847493 -0.024727648 ## homeOwnershipOWN -0.045806272 -0.4334673269 -0.589004430 ## homeOwnershipRENT -0.618552181 0.0302284478 0.313543643 ## empLength< 1 year -0.001797284 0.4576296418 -0.058412021 ## empLength1 year -0.130963768 0.0422715290 0.055130890 ## empLength10+ years 0.287725982 -0.6416129898 0.363060034 ## empLength2 years -0.128941170 0.0763491979 0.039146531 ## empLength3 years -0.099034453 0.0770658747 0.008655596 ## empLength4 years -0.069874577 0.0751682623 -0.003951253 ## empLength5 years -0.044999307 0.0863378060 -0.037230429 ## empLength6 years -0.018064151 0.1038335394 -0.007034248 ## empLength7 years -0.011167098 0.0871645368 -0.027346571 ## empLength8 years -0.003164955 0.0746941318 -0.032580385 ## empLength9 years 0.008960206 0.0919692402 -0.052628954 ## empLengthNA -0.023907131 -0.0211256891 -0.570980121","title":"Clustering & PCA"},{"location":"LCDA_Part2/#importing-packages","text":"library(dplyr) library(dummies) # dummy variables library(cluster) # Clustering library(rgl) # 3d plot load(\"LoanStats2017Q3.rda\") set.seed(12345)","title":"Importing packages"},{"location":"LCDA_Part2/#preparation-for-clustering","text":"Storing variables in vectors: annualInc <- lendingData$annual_inc loan_amnt <- lendingData$loan_amnt empLength <- lendingData$emp_length homeOwnership <- lendingData$home_ownership dti <- lendingData$dti grade <- lendingData$grade Removing the value 'n/a' from emp_length column and replacing it with NA: empLength[empLength == 'n/a'] <- NA empLength <- addNA(empLength) Storing all variables in a data frame: tempDF <- data.frame(annualInc, loan_amnt, dti, homeOwnership, empLength) Checking for NULL values in the data frame and then imputing them with the median: apply(is.na(tempDF),2,sum) ## annualInc loan_amnt dti homeOwnership empLength ## 0 0 184 0 0 tempDF$dti[is.na(tempDF$dti)] <- median(tempDF$dti, na.rm=TRUE) head(tempDF) ## annualInc loan_amnt dti homeOwnership empLength ## 1 42000 12000 27.74 OWN 10+ years ## 2 79077 16000 15.94 RENT 5 years ## 3 107000 33000 19.06 MORTGAGE < 1 year ## 4 155000 32000 12.35 MORTGAGE 10+ years ## 5 120000 40000 31.11 MORTGAGE 9 years ## 6 32000 7000 12.27 RENT 10+ years Creating a dummy data frame with categorical columns: lendDF <- dummy.data.frame(tempDF, names=c(\"homeOwnership\", \"empLength\")) head(lendDF) ## annualInc loan_amnt dti homeOwnershipANY homeOwnershipMORTGAGE ## 1 42000 12000 27.74 0 0 ## 2 79077 16000 15.94 0 0 ## 3 107000 33000 19.06 0 1 ## 4 155000 32000 12.35 0 1 ## 5 120000 40000 31.11 0 1 ## 6 32000 7000 12.27 0 0 ## homeOwnershipNONE homeOwnershipOWN homeOwnershipRENT empLength< 1 year ## 1 0 1 0 0 ## 2 0 0 1 0 ## 3 0 0 0 1 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 1 0 ## empLength1 year empLength10+ years empLength2 years empLength3 years ## 1 0 1 0 0 ## 2 0 0 0 0 ## 3 0 0 0 0 ## 4 0 1 0 0 ## 5 0 0 0 0 ## 6 0 1 0 0 ## empLength4 years empLength5 years empLength6 years empLength7 years ## 1 0 0 0 0 ## 2 0 1 0 0 ## 3 0 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## empLength8 years empLength9 years empLengthNA ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 1 0 ## 6 0 0 0 Scaling the values in the data frame for clustering: lendDF <- scale(lendDF, center=TRUE, scale=TRUE) head(lendDF) ## annualInc loan_amnt dti homeOwnershipANY homeOwnershipMORTGAGE ## 1 -0.1149444060 -0.2702472 0.39399388 -0.008074829 -0.9699266 ## 2 0.0002058191 0.1458219 -0.18034054 -0.008074829 -0.9699266 ## 3 0.0869264204 1.9141156 -0.02848262 -0.008074829 1.0309975 ## 4 0.2360002614 1.8100983 -0.35507448 -0.008074829 1.0309975 ## 5 0.1273005856 2.6422365 0.55801989 -0.008074829 1.0309975 ## 6 -0.1460014562 -0.7903336 -0.35896827 -0.008074829 -0.9699266","title":"Preparation for Clustering"},{"location":"LCDA_Part2/#pca","text":"Performing K-means clustering: lendKmeans <- kmeans(lendDF, centers=7) lendPCA <- prcomp(lendDF, retx=TRUE) plot(lendPCA$x[,1:2], col=lendKmeans$cluster, pch=lendKmeans$cluster) legend(\"topright\", title = ,legend = 1:7, col = 1:7, pch = 1:7) Rotation matrix for first 2 Principal components: lendPCA$rotation[,1:2] ## PC1 PC2 ## annualInc 0.061042681 -0.016995412 ## loan_amnt 0.269090193 0.179204905 ## dti 0.063038509 0.195250523 ## homeOwnershipANY -0.003757898 0.005487485 ## homeOwnershipMORTGAGE 0.634478287 0.247075768 ## homeOwnershipNONE -0.001164800 -0.001968247 ## homeOwnershipOWN -0.046104282 -0.423947441 ## homeOwnershipRENT -0.617114633 0.023901256 ## empLength< 1 year -0.002269023 0.458873341 ## empLength1 year -0.130646715 0.040961336 ## empLength10+ years 0.287971605 -0.647021527 ## empLength2 years -0.128632358 0.075200216 ## empLength3 years -0.098845568 0.076617349 ## empLength4 years -0.069703845 0.074853769 ## empLength5 years -0.044876646 0.086490324 ## empLength6 years -0.017936943 0.103554449 ## empLength7 years -0.011065154 0.087224445 ## empLength8 years -0.003133917 0.074989097 ## empLength9 years 0.009005405 0.092448604 ## empLengthNA -0.025108522 -0.010037250 summary(lendPCA) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 1.40804 1.1428 1.12931 1.06944 1.04506 1.04341 1.03802 ## Proportion of Variance 0.09913 0.0653 0.06377 0.05719 0.05461 0.05444 0.05387 ## Cumulative Proportion 0.09913 0.1644 0.22820 0.28538 0.33999 0.39442 0.44830","title":"PCA"},{"location":"LCDA_Part2/#pca-description","text":"After inspecting the first two PCs the observations are as follows: \u2022 The variation on the first component is due to several related variables concerning the borrowers\u2019 loan amount, homes that are mortgaged and borrowers employed for more than 10 years. Borrowers with larger values on the first component will have larger values for these variables. \u2022 The variation on the second component is due to the borrowers employed for less than a year and for 6 years, homes that are mortgaged, and debt-to-income ratio. Borrowers with larger values on the second component will have larger values for these variables. \u2022 The black cluster has negative values for PC1, indicating fewer rented homes and lesser number of borrowers employed between 1 to 4 years inclusive. The scores on PC2 are negative as well, indicating borrowers with lower annual income. \u2022 The pink cluster has positive values for PC1 than the black cluster , indicating a greater number of borrowers whose homes are mortgaged, have higher loan amounts and have been employed for more than 10 years. The scores on PC2 are near zero, indicating that the values are moderate compared to the rest of the data. \u2022 The yellow clusters & cyan clusters overlap with the black cluster and the red cluster & green cluster clusters overlap with pink cluster . This indicates that the overlapping clusters have similar characteristics. \u2022 The blue cluster has the largest positive values for PC2 corresponding to larger number of borrowers with employment term < 1 year, higher mortgaged homes, higher debt-to-income ratio and higher annual income. \u2022 In PC2, there is a trade-off between more borrowers with mortgaged homes and lesser owned homes. Another trade-off between more borrowers with lower employment length (< 1 year) and lower number of borrowers employed for 10+ years. \u2022 In PC1, there is a trade-off between mortgaged homes and lower number of borrowers with rented homes. Another trade-off between more borrowers employed (>10 years) and less borrowers employed from 1 to 4 years. \u2022 The outlier seems to affect strongly the first principal component.","title":"PCA description"},{"location":"LCDA_Part2/#outlier-detection","text":"#lendPCA$x[,1] which(lendPCA$x[,1] > 20) ## 72164 ## 72164 lendPCA$x[72164, 1] ## [1] 20.42088 Running Kmeans again and replotting: lendDF <- lendDF[-72164,] lendKmeans <- kmeans(lendDF, centers=7) lendPCA <- prcomp(lendDF, retx=TRUE) plot(lendPCA$x[,1:2], col=lendKmeans$cluster, pch=lendKmeans$cluster) legend(\"topright\", title = ,legend = 1:7, col = 1:7, pch = 1:7) plot3d(lendPCA$x[,1:3], col=lendKmeans$cluster, pch=lendKmeans$cluster) lendPCA$rotation[,1:3] Below is the PCA rotation matrix for first 3 PCs. \u2022 PC3 is associated with borrowers\u2019 loan amount, rented homes and employment length more than 10 years. \u2022 The positive variation in the third PC has to do with borrowers with higher rented homes, loan amount and number of borrowers with over 10years of employment. \u2022 The negative variation in the third PC has to do with lower debt-to-income ratio and lesser number of borrowers with own homes. ## PC1 PC2 PC3 ## annualInc 0.031617716 -0.0008315871 0.028532744 ## loan_amnt 0.267672066 0.1832264396 0.115086885 ## dti 0.064235286 0.1896985757 -0.249456961 ## homeOwnershipANY -0.003734964 0.0049164790 -0.033920042 ## homeOwnershipMORTGAGE 0.635696955 0.2469606171 0.069240871 ## homeOwnershipNONE -0.001156300 -0.0023847493 -0.024727648 ## homeOwnershipOWN -0.045806272 -0.4334673269 -0.589004430 ## homeOwnershipRENT -0.618552181 0.0302284478 0.313543643 ## empLength< 1 year -0.001797284 0.4576296418 -0.058412021 ## empLength1 year -0.130963768 0.0422715290 0.055130890 ## empLength10+ years 0.287725982 -0.6416129898 0.363060034 ## empLength2 years -0.128941170 0.0763491979 0.039146531 ## empLength3 years -0.099034453 0.0770658747 0.008655596 ## empLength4 years -0.069874577 0.0751682623 -0.003951253 ## empLength5 years -0.044999307 0.0863378060 -0.037230429 ## empLength6 years -0.018064151 0.1038335394 -0.007034248 ## empLength7 years -0.011167098 0.0871645368 -0.027346571 ## empLength8 years -0.003164955 0.0746941318 -0.032580385 ## empLength9 years 0.008960206 0.0919692402 -0.052628954 ## empLengthNA -0.023907131 -0.0211256891 -0.570980121","title":"Outlier detection"},{"location":"LCDA_Part3/","text":"Building a robust model using Logistic Regression & Classification trees Importing Packages library(dplyr) library(rpart) # Classification tree library(caret) # classification & regression library(ROCR) # ROC curve load(\"LoanStats2017Q3.rda\") Changing format of all date columns in the original data set and then creating a data frame \u2018myLDF\u2019 to store the dependent variable and the independent variables considered during loan approvals. Dependent Variable: loan_status # Specifying date columns dateColumns <- c(\"issue_d\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\", \"sec_app_earliest_cr_line\", \"hardship_end_date\", \"hardship_start_date\", \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\", \"earliest_cr_line\") # Formating the date columns lendingData[,dateColumns] <- lapply(lendingData[,dateColumns], as.POSIXct) myLDF <- lendingData %>% dplyr::select(\"loan_status\", \"issue_d\", \"last_pymnt_d\", \"last_credit_pull_d\", \"sec_app_earliest_cr_line\", \"hardship_end_date\", \"hardship_start_date\", \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\", \"annual_inc\", \"emp_length\", \"mths_since_recent_inq\",\"num_op_rev_tl\", \"pub_rec\", \"fico_range_high\", \"fico_range_low\", \"num_sats\", \"open_acc\", \"pub_rec_bankruptcies\", \"pub_rec\", \"purpose\") %>% dplyr:: filter(loan_status %in% c(\"Fully Paid\", \"Charged Off\")) %>% droplevels() Splitting the data frame \u201cmyLDF\u201d into two sets: Training & test, where training set will have 20% of the rows and test set will have 80% of the rows for analysis: trainRows <- createDataPartition(myLDF$loan_status, p = 0.2, list=FALSE) myLDFtrain <- myLDF[trainRows,] myLDFtest <- myLDF[-trainRows,] Before moving on, we need to check for NULLs. All the columns with NULL values have to be ignored. # Looking for NULLs in training dataset apply(is.na(myLDFtrain),2,sum) # Imputing NULLs with the median in the training dataset myLDFtrain$mths_since_recent_inq[is.na(myLDFtrain$mths_since_recent_inq)] <- median(myLDFtrain$mths_since_recent_inq, na.rm=TRUE) myLDFtrain$sec_app_earliest_cr_line[is.na(myLDFtrain$sec_app_earliest_cr_line)] <- median(myLDFtrain$sec_app_earliest_cr_line, na.rm=TRUE) myLDFtrain$hardship_end_date[is.na(myLDFtrain$hardship_end_date)] <- median(myLDFtrain$hardship_end_date, na.rm=TRUE) myLDFtrain$hardship_start_date[is.na(myLDFtrain$hardship_start_date)] <- median(myLDFtrain$hardship_start_date, na.rm=TRUE) myLDFtrain$payment_plan_start_date[is.na(myLDFtrain$payment_plan_start_date)] <- median(myLDFtrain$payment_plan_start_date, na.rm=TRUE) myLDFtrain$debt_settlement_flag_date[is.na(myLDFtrain$debt_settlement_flag_date)] <- median(myLDFtrain$debt_settlement_flag_date, na.rm=TRUE) myLDFtrain$last_pymnt_d[is.na(myLDFtrain$last_pymnt_d)] <- median(myLDFtrain$last_pymnt_d, na.rm=TRUE) myLDFtrain$settlement_date[is.na(myLDFtrain$settlement_date)] <- median(myLDFtrain$settlement_date, na.rm=TRUE) ## loan_status issue_d last_pymnt_d ## 0 10671 10671 ## last_credit_pull_d sec_app_earliest_cr_line hardship_end_date ## 10671 10671 10671 ## hardship_start_date payment_plan_start_date debt_settlement_flag_date ## 10671 10671 10671 ## settlement_date annual_inc emp_length ## 10671 0 0 ## mths_since_recent_inq num_op_rev_tl pub_rec ## 1006 0 0 ## fico_range_high fico_range_low num_sats ## 0 0 0 ## open_acc pub_rec_bankruptcies purpose ## 0 0 0 # Imputing NULLs with the median of the training set in the test dataset myLDFtest$mths_since_recent_inq[is.na(myLDFtest$mths_since_recent_inq)] <- median(myLDFtrain$mths_since_recent_inq, na.rm=TRUE) myLDFtest$sec_app_earliest_cr_line[is.na(myLDFtest$sec_app_earliest_cr_line)] <- median(myLDFtrain$sec_app_earliest_cr_line, na.rm=TRUE) myLDFtest$hardship_end_date[is.na(myLDFtest$hardship_end_date)] <- median(myLDFtrain$hardship_end_date, na.rm=TRUE) myLDFtest$hardship_start_date[is.na(myLDFtest$hardship_start_date)] <- median(myLDFtrain$hardship_start_date, na.rm=TRUE) myLDFtest$payment_plan_start_date[is.na(myLDFtest$payment_plan_start_date)] <- median(myLDFtrain$payment_plan_start_date, na.rm=TRUE) myLDFtest$debt_settlement_flag_date[is.na(myLDFtest$debt_settlement_flag_date)] <- median(myLDFtrain$debt_settlement_flag_date, na.rm=TRUE) myLDFtest$last_pymnt_d[is.na(myLDFtest$last_pymnt_d)] <- median(myLDFtrain$last_pymnt_d, na.rm=TRUE) myLDFtest$settlement_date[is.na(myLDFtest$settlement_date)] <- median(myLDFtrain$settlement_date, na.rm=TRUE) Since, we need to find the final loan status and for logistic regression we need two significant output values: \u201cFully Paid\u201d & \u201cCharged Off\u201d. Then we can check for the total number of \u201cFully Charged\u201d & \u201cFully Paid\u201d and assigning weights accordingly; sum(myLDFtrain$loan_status == \"Fully Paid\") sum(myLDFtrain$loan_status == \"Charged Off\") ## 8095 sum(myLDFtrain$loan_status == \"Charged Off\") ## 2576 # We can now assign weights by making it thrice as expensive to misclassify myLendWeights <- numeric(nrow(myLDFtrain)) myLendWeights[myLDFtrain$loan_stat == \"Fully Paid\"] <- 1 myLendWeights[myLDFtrain$loan_stat == \"Charged Off\"] <- 3 Model-1 Logistic Regression Running logistic regression using glm function on the training set and summarizing the output. Here we can consider the default significance level to be 0.05. The following variables don\u2019t seem to be significant since their p values are less than 0.05. \u2022 sec_app_earliest_cr_line, \u2022 hardship_end_date, emp_length, \u2022 num_op_rev_tl, pub_rec, \u2022 fico_range_high, \u2022 fico_range_low, \u2022 num_sats, \u2022 open_acc, \u2022 pub_rec_bankruptcies myLendLR <- glm(loan_status ~ ., data=myLDFtrain, weights=myLendWeights, family=binomial(\"logit\")) summary(myLendLR) Now can predict the outcome (using predict function) using the test set where we obtain probabilities and store them in \u2018myLendpredict\u2019. For calculating the confusion matrix, we need to divide the list of predicted probabilities in \u2018myLendpredict\u2019 into 2 groups: \u201c< 0.5 as Charged Off & >= 0.5 as Fully Paid\u201d. # all probabilities as logistic regression myLendPredict <- predict(myLendLR, newdata=myLDFtest, type=\"response\") # Classification matrix for logistic regression myLendPredictLR <- character(length(myLendPredict)) myLendPredictLR[myLendPredict < 0.5] <- \"Charged Off\" myLendPredictLR[myLendPredict >= 0.5] <- \"Fully Paid\" length(myLendPredict) myLendLRCM <- table(myLDFtest$loan_status, myLendPredictLR) # confusion matrix from test data # Classification rate for logistic regresssion 1-sum(diag(myLendLRCM))/sum(myLendLRCM) exp(coef(myLendLR)) myLendPredictLR Charged Off Fully Paid Charged Off 6205 4095 Fully Paid 12331 20045 Misclassification rate: 0.3849 Model-2 Classification trees For classification tree, we can create a data frame \u2018lendrpart\u2019 and use function \u2018rpart\u2019 and plotting the tree. set.seed(123) lendrpart <- rpart(loan_status ~ ., data=myLDFtrain, weights = myLendWeights) head(lendrpart) lendrpart$variable.importance #Plot the model plot(lendrpart, uniform=TRUE,margin=0.01, asp = 0) text(lendrpart, cex=.6) # Classification matrix for rpart myPredictrpart <- predict(lendrpart, newdata=myLDFtest, type=\"class\") myLendrpartCM <- table(myLDFtest$loan_status, myPredictrpart) myLendrpartCM 1-sum(diag(myLendrpartCM))/sum(myLendrpartCM) myPredictrpart Charged Off Fully Paid Charged Off 6308 3992 Fully Paid 7856 24520 Misclassification rate: 0.2776 ROC curves for logistic regression & classification trees The following variables seem to be more important in predicting the outcome using logistic regression since their p-values are less than 0.05; \u2022 issue_d, \u2022 last_pymnt_d, \u2022 last_credit_pull_d, \u2022 hardship_start_date, \u2022 payment_plan_start_date, \u2022 debt_settlement_flag_date, \u2022 settlement_date, annual_inc, \u2022 mths_since_recent_inq, \u2022 purpose last_pymnt_d last_credit_pull_d fico_range_high fico_range_low settlement_date 763.0743221 219.6862028 188.5453735 188.5453735 170.6381274 debt_settlement_flag_date 146.6735896 # ROC Curve Model 1 myLRPredict <- predict(myLendLR, myLDFtest, type=\"response\") myLRPred <- prediction(myLRPredict, myLDFtest$loan_status, label.ordering=c(\"Charged Off\", \"Fully Paid\")) myLRPerf <- performance(myLRPred, \"tpr\", \"fpr\") # ROC curve Model 2 myrpartPredict <- predict(lendrpart, myLDFtest, type=\"prob\") myrpartPredict # Only need positive for the plot hence '2' myrpartPred <- prediction(myrpartPredict[,2], myLDFtest$loan_status, label.ordering=c(\"Charged Off\", \"Fully Paid\")) myrpartPerf <- performance(myrpartPred, \"tpr\", \"fpr\") Models with additional variables: Let's try building a logistic regression model and a classification tree model for predicting the final status of a loan based on the following variables using the same training and testing observation: \u2022 loan_amnt, funded_amnt_inv, term, int_rate, installment, grade, emp_length, home_ownership, annual_inc, verification_status, loan_status, purpose, title, dti, total_pymnt, delinq_2yrs, open_acc, pub_rec, last_pymnt_d, last_pymnt_amnt, application_type, revol_bal, revol_util, recoveries. The \u2018home_ownership\u2019 is a categorical variable with \u2018NA\u2019 values. Before proceeding, we need to filter out only the important values: \u2018Mortgage\u2019, \u2018Own\u2019 & \u2018Rent\u2019. # filtering only 3 values from the column lendingData <- lendingData %>% dplyr::filter(home_ownership %in% c(\"OWN\", \"RENT\", \"MORTGAGE\")) %>% droplevels() #levels(lendingData) <- c(\"MORTGAGE\", \"MORTGAGE\", \"MORTGAGE\", \"OWN\", \"RENT\") myL2DF <- lendingData %>% dplyr::select(\"loan_status\", \"annual_inc\", \"loan_amnt\",\"term\", \"funded_amnt_inv\", \"int_rate\", \"installment\", \"grade\", \"verification_status\", \"title\", \"dti\",\"total_pymnt\", \"delinq_2yrs\", \"last_pymnt_d\", \"last_pymnt_amnt\", \"application_type\",\"revol_bal\", \"revol_util\", \"emp_length\", \"pub_rec\", \"home_ownership\", \"recoveries\", \"open_acc\", \"pub_rec\", \"purpose\", \"title\", \"verification_status\") %>% dplyr:: filter(loan_status %in% c(\"Fully Paid\", \"Charged Off\")) %>% droplevels() table(lendingData$home_ownership) apply(is.na(myL2DF),2,sum) Dividing the data frame \u201cmyL2DF\u201d into two sets: Training & test, where training set will have 20% of the rows and test set will have 80% of the rows for analysis. Before moving on, we need to check for NULLs. All the columns with NULL values have to be ignored. trainRows2 <- createDataPartition(myL2DF$loan_status, p = 0.2, list=FALSE) trainRows2 myL2DFtrain <- myL2DF[trainRows2,] myL2DFtest <- myL2DF[-trainRows2,] # Imputing median in place of NULLs myL2DFtrain$last_pymnt_d[is.na(myL2DFtrain$last_pymnt_d)] <- median(myL2DFtrain$last_pymnt_d, na.rm=TRUE) myL2DFtrain$revol_util[is.na(myL2DFtrain$revol_util)] <- median(myL2DFtrain$revol_util, na.rm=TRUE) myL2DFtrain$dti[is.na(myL2DFtrain$dti)] <- median(myL2DFtrain$dti, na.rm=TRUE) myL2DFtest$last_pymnt_d[is.na(myL2DFtest$last_pymnt_d)] <- median(myL2DFtrain$last_pymnt_d, na.rm=TRUE) myL2DFtest$revol_util[is.na(myL2DFtest$revol_util)] <- median(myL2DFtrain$revol_util, na.rm=TRUE) myL2DFtest$dti[is.na(myL2DFtest$dti)] <- median(myL2DFtrain$dti, na.rm=TRUE) Since, we need to find the final loan status and for logistic regression we need two significant output values: \u201cFully Paid\u201d & \u201cCharged Off\u201d. Then we can check for the total number of \u201cFully Charged\u201d & \u201cFully Paid\u201d and assigning weights accordingly; sum(myL2DFtrain$loan_status == \"Fully Paid\") ## 8094 sum(myL2DFtrain$loan_status == \"Charged Off\") ## 2575 sum(myL2DFtrain$loan_status == \"Fully Paid\") sum(myL2DFtrain$loan_status == \"Charged Off\") myLendWeights2 <- numeric(nrow(myL2DFtrain)) myLendWeights2[myL2DFtrain$loan_status == \"Fully Paid\"] <- 1 myLendWeights2[myL2DFtrain$loan_status == \"Charged Off\"] <- 3 Model-3 Logistic Regression Running logistic regression using \u2018glm\u2019 function on the training set and summarizing the output. Then using the predict function on the test set for classifications based on the responses. Splitting these classes into two groups for the confusion matrix: \u201c< 0.5 as Charged Off & >= 0.5 as Fully Paid\u201d. myLendLR2 <- glm(loan_status ~ ., data=myL2DFtrain, weights=myLendWeights2, family=binomial(\"logit\")) summary(myLendLR2) # all probabilities as logistic regression myLendPredict2 <- predict(myLendLR2, newdata=myL2DFtest, type=\"response\") summary(myLendPredict2) # confusion matrix from test data myLendPredictLR2 <- character(length(myLendPredict2)) myLendPredictLR2[myLendPredict2 < 0.5] <- \"Charged Off\" myLendPredictLR2[myLendPredict2 >= 0.5] <- \"Fully Paid\" myLendLR2CM <- table(myL2DFtest$loan_status, myLendPredictLR2) myLendLR2CM 1-sum(diag(myLendLR2CM))/sum(myLendLR2CM) summary(myLendLR2) exp(coef(myLendLR2)) myLendPredictLR2 Charged Off Fully Paid Charged Off 10238 62 Fully Paid 16 32357 Misclassification rate: 0.001827854 Model-4 Classification trees lendrpart2 <- rpart(loan_status ~ ., data=myL2DFtrain, weights = myLendWeights2) head(lendrpart2) #Plot the model plot(lendrpart2, uniform=TRUE,margin=0.01, asp = 0) text(lendrpart2, cex=.6) # Confusion matrix myPredictrpart2 <- predict(lendrpart2, newdata=myL2DFtest, type=\"class\") myPredictrpart2 myLendrpart2CM <- table(myL2DFtest$loan_status, myPredictrpart2) myLendrpart2CM 1-sum(diag(myLendrpart2CM))/sum(myLendrpart2CM) # misclassification rate myPredictrpart2 Charged Off Fully Paid Charged Off 9874 426 Fully Paid 1605 30768 Misclassification rate: 0.0475945 # ROC curve for logistic myLRPredict2 <- predict(myLendLR2, myL2DFtest, type=\"response\") myLRPred2 <- prediction(myLRPredict2, myL2DFtest$loan_status, label.ordering=c(\"Charged Off\", \"Fully Paid\")) myLRPerf2 <- performance(myLRPred2, \"tpr\", \"fpr\") # ROC curve for classification myrpartPredict2 <- predict(lendrpart2, myL2DFtest, type=\"prob\") # Only need positive for the plot hence '2' myrpartPred2 <- prediction(myrpartPredict2[,2], myL2DFtest$loan_status, label.ordering=c(\"Charged Off\", \"Fully Paid\")) myrpartPerf2 <- performance(myrpartPred2, \"tpr\", \"fpr\") ROC curves plot(myrpartPerf2, col=18) plot(myLRPerf2, col=19, add=TRUE) plot(myrpartPerf, col=20, add=TRUE) plot(myLRPerf, col=21, add=TRUE) legend(0.5, 0.6, c(\"Log. Reg.1\", \"Class. Tree1\", \"Log. Reg.2\", \"Class. Tree2\"), col=c(21, 20, 19,18), lwd=3) Area Under Curve performance(myLRPred, \"auc\") performance(myrpartPred, \"auc\") performance(myLRPred2, \"auc\") performance(myrpartPred2, \"auc\") Area under the ROC curve for model 1(Logistic regression): 0.6486718 Area under the ROC curve for model 2(Classification): 0.7584767 The curve for classification tree (model 2) is slightly higher than model 1, indicating a better performance across the different choices for false positive rate. At a false positive rate of 0.4 (so the true negative rate is 0.6), the true positive rate is about 0.8. Area under the ROC curve for model 3(Logistic regression): 0.9987902 Area under the ROC curve for model 4(Classification): 0.9890251 The curve for logistic regression (model 3) is slightly higher than model 4 and much higher than models 1 & 2, indicating good performance across the different choices for false positive rate. At a false positive rate of 0 (so the true negative rate is 1), the true positive rate is 1. It might seem that model 3 is the best model since it has the highest area under the curve and the lowest misclassification rate. However, using this loan performance data (that consists of variables considered during loan approval) will result in our model performing better with data than in actual practice because these variables are not known at the time of investment. The differences in performance between the models in Q #2 and Q #3 can be attributed to data leakage . It happens when information from outside the training dataset is used to create the model. Data leakage can lead to the creation overly optimistic predictive models. Leakage can be detected if we get performance that seems a little too good to be true like in model 3. The best model in my opinion is model 2 (Classification tree - dark blue line). It has AUC: 0.7584767.","title":"Building Models"},{"location":"LCDA_Part3/#importing-packages","text":"library(dplyr) library(rpart) # Classification tree library(caret) # classification & regression library(ROCR) # ROC curve load(\"LoanStats2017Q3.rda\") Changing format of all date columns in the original data set and then creating a data frame \u2018myLDF\u2019 to store the dependent variable and the independent variables considered during loan approvals. Dependent Variable: loan_status # Specifying date columns dateColumns <- c(\"issue_d\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\", \"sec_app_earliest_cr_line\", \"hardship_end_date\", \"hardship_start_date\", \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\", \"earliest_cr_line\") # Formating the date columns lendingData[,dateColumns] <- lapply(lendingData[,dateColumns], as.POSIXct) myLDF <- lendingData %>% dplyr::select(\"loan_status\", \"issue_d\", \"last_pymnt_d\", \"last_credit_pull_d\", \"sec_app_earliest_cr_line\", \"hardship_end_date\", \"hardship_start_date\", \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\", \"annual_inc\", \"emp_length\", \"mths_since_recent_inq\",\"num_op_rev_tl\", \"pub_rec\", \"fico_range_high\", \"fico_range_low\", \"num_sats\", \"open_acc\", \"pub_rec_bankruptcies\", \"pub_rec\", \"purpose\") %>% dplyr:: filter(loan_status %in% c(\"Fully Paid\", \"Charged Off\")) %>% droplevels() Splitting the data frame \u201cmyLDF\u201d into two sets: Training & test, where training set will have 20% of the rows and test set will have 80% of the rows for analysis: trainRows <- createDataPartition(myLDF$loan_status, p = 0.2, list=FALSE) myLDFtrain <- myLDF[trainRows,] myLDFtest <- myLDF[-trainRows,] Before moving on, we need to check for NULLs. All the columns with NULL values have to be ignored. # Looking for NULLs in training dataset apply(is.na(myLDFtrain),2,sum) # Imputing NULLs with the median in the training dataset myLDFtrain$mths_since_recent_inq[is.na(myLDFtrain$mths_since_recent_inq)] <- median(myLDFtrain$mths_since_recent_inq, na.rm=TRUE) myLDFtrain$sec_app_earliest_cr_line[is.na(myLDFtrain$sec_app_earliest_cr_line)] <- median(myLDFtrain$sec_app_earliest_cr_line, na.rm=TRUE) myLDFtrain$hardship_end_date[is.na(myLDFtrain$hardship_end_date)] <- median(myLDFtrain$hardship_end_date, na.rm=TRUE) myLDFtrain$hardship_start_date[is.na(myLDFtrain$hardship_start_date)] <- median(myLDFtrain$hardship_start_date, na.rm=TRUE) myLDFtrain$payment_plan_start_date[is.na(myLDFtrain$payment_plan_start_date)] <- median(myLDFtrain$payment_plan_start_date, na.rm=TRUE) myLDFtrain$debt_settlement_flag_date[is.na(myLDFtrain$debt_settlement_flag_date)] <- median(myLDFtrain$debt_settlement_flag_date, na.rm=TRUE) myLDFtrain$last_pymnt_d[is.na(myLDFtrain$last_pymnt_d)] <- median(myLDFtrain$last_pymnt_d, na.rm=TRUE) myLDFtrain$settlement_date[is.na(myLDFtrain$settlement_date)] <- median(myLDFtrain$settlement_date, na.rm=TRUE) ## loan_status issue_d last_pymnt_d ## 0 10671 10671 ## last_credit_pull_d sec_app_earliest_cr_line hardship_end_date ## 10671 10671 10671 ## hardship_start_date payment_plan_start_date debt_settlement_flag_date ## 10671 10671 10671 ## settlement_date annual_inc emp_length ## 10671 0 0 ## mths_since_recent_inq num_op_rev_tl pub_rec ## 1006 0 0 ## fico_range_high fico_range_low num_sats ## 0 0 0 ## open_acc pub_rec_bankruptcies purpose ## 0 0 0 # Imputing NULLs with the median of the training set in the test dataset myLDFtest$mths_since_recent_inq[is.na(myLDFtest$mths_since_recent_inq)] <- median(myLDFtrain$mths_since_recent_inq, na.rm=TRUE) myLDFtest$sec_app_earliest_cr_line[is.na(myLDFtest$sec_app_earliest_cr_line)] <- median(myLDFtrain$sec_app_earliest_cr_line, na.rm=TRUE) myLDFtest$hardship_end_date[is.na(myLDFtest$hardship_end_date)] <- median(myLDFtrain$hardship_end_date, na.rm=TRUE) myLDFtest$hardship_start_date[is.na(myLDFtest$hardship_start_date)] <- median(myLDFtrain$hardship_start_date, na.rm=TRUE) myLDFtest$payment_plan_start_date[is.na(myLDFtest$payment_plan_start_date)] <- median(myLDFtrain$payment_plan_start_date, na.rm=TRUE) myLDFtest$debt_settlement_flag_date[is.na(myLDFtest$debt_settlement_flag_date)] <- median(myLDFtrain$debt_settlement_flag_date, na.rm=TRUE) myLDFtest$last_pymnt_d[is.na(myLDFtest$last_pymnt_d)] <- median(myLDFtrain$last_pymnt_d, na.rm=TRUE) myLDFtest$settlement_date[is.na(myLDFtest$settlement_date)] <- median(myLDFtrain$settlement_date, na.rm=TRUE) Since, we need to find the final loan status and for logistic regression we need two significant output values: \u201cFully Paid\u201d & \u201cCharged Off\u201d. Then we can check for the total number of \u201cFully Charged\u201d & \u201cFully Paid\u201d and assigning weights accordingly; sum(myLDFtrain$loan_status == \"Fully Paid\") sum(myLDFtrain$loan_status == \"Charged Off\") ## 8095 sum(myLDFtrain$loan_status == \"Charged Off\") ## 2576 # We can now assign weights by making it thrice as expensive to misclassify myLendWeights <- numeric(nrow(myLDFtrain)) myLendWeights[myLDFtrain$loan_stat == \"Fully Paid\"] <- 1 myLendWeights[myLDFtrain$loan_stat == \"Charged Off\"] <- 3","title":"Importing Packages"},{"location":"LCDA_Part3/#model-1-logistic-regression","text":"Running logistic regression using glm function on the training set and summarizing the output. Here we can consider the default significance level to be 0.05. The following variables don\u2019t seem to be significant since their p values are less than 0.05. \u2022 sec_app_earliest_cr_line, \u2022 hardship_end_date, emp_length, \u2022 num_op_rev_tl, pub_rec, \u2022 fico_range_high, \u2022 fico_range_low, \u2022 num_sats, \u2022 open_acc, \u2022 pub_rec_bankruptcies myLendLR <- glm(loan_status ~ ., data=myLDFtrain, weights=myLendWeights, family=binomial(\"logit\")) summary(myLendLR) Now can predict the outcome (using predict function) using the test set where we obtain probabilities and store them in \u2018myLendpredict\u2019. For calculating the confusion matrix, we need to divide the list of predicted probabilities in \u2018myLendpredict\u2019 into 2 groups: \u201c< 0.5 as Charged Off & >= 0.5 as Fully Paid\u201d. # all probabilities as logistic regression myLendPredict <- predict(myLendLR, newdata=myLDFtest, type=\"response\") # Classification matrix for logistic regression myLendPredictLR <- character(length(myLendPredict)) myLendPredictLR[myLendPredict < 0.5] <- \"Charged Off\" myLendPredictLR[myLendPredict >= 0.5] <- \"Fully Paid\" length(myLendPredict) myLendLRCM <- table(myLDFtest$loan_status, myLendPredictLR) # confusion matrix from test data # Classification rate for logistic regresssion 1-sum(diag(myLendLRCM))/sum(myLendLRCM) exp(coef(myLendLR)) myLendPredictLR Charged Off Fully Paid Charged Off 6205 4095 Fully Paid 12331 20045 Misclassification rate: 0.3849","title":"Model-1 Logistic Regression"},{"location":"LCDA_Part3/#model-2-classification-trees","text":"For classification tree, we can create a data frame \u2018lendrpart\u2019 and use function \u2018rpart\u2019 and plotting the tree. set.seed(123) lendrpart <- rpart(loan_status ~ ., data=myLDFtrain, weights = myLendWeights) head(lendrpart) lendrpart$variable.importance #Plot the model plot(lendrpart, uniform=TRUE,margin=0.01, asp = 0) text(lendrpart, cex=.6) # Classification matrix for rpart myPredictrpart <- predict(lendrpart, newdata=myLDFtest, type=\"class\") myLendrpartCM <- table(myLDFtest$loan_status, myPredictrpart) myLendrpartCM 1-sum(diag(myLendrpartCM))/sum(myLendrpartCM) myPredictrpart Charged Off Fully Paid Charged Off 6308 3992 Fully Paid 7856 24520 Misclassification rate: 0.2776 ROC curves for logistic regression & classification trees The following variables seem to be more important in predicting the outcome using logistic regression since their p-values are less than 0.05; \u2022 issue_d, \u2022 last_pymnt_d, \u2022 last_credit_pull_d, \u2022 hardship_start_date, \u2022 payment_plan_start_date, \u2022 debt_settlement_flag_date, \u2022 settlement_date, annual_inc, \u2022 mths_since_recent_inq, \u2022 purpose last_pymnt_d last_credit_pull_d fico_range_high fico_range_low settlement_date 763.0743221 219.6862028 188.5453735 188.5453735 170.6381274 debt_settlement_flag_date 146.6735896 # ROC Curve Model 1 myLRPredict <- predict(myLendLR, myLDFtest, type=\"response\") myLRPred <- prediction(myLRPredict, myLDFtest$loan_status, label.ordering=c(\"Charged Off\", \"Fully Paid\")) myLRPerf <- performance(myLRPred, \"tpr\", \"fpr\") # ROC curve Model 2 myrpartPredict <- predict(lendrpart, myLDFtest, type=\"prob\") myrpartPredict # Only need positive for the plot hence '2' myrpartPred <- prediction(myrpartPredict[,2], myLDFtest$loan_status, label.ordering=c(\"Charged Off\", \"Fully Paid\")) myrpartPerf <- performance(myrpartPred, \"tpr\", \"fpr\") Models with additional variables: Let's try building a logistic regression model and a classification tree model for predicting the final status of a loan based on the following variables using the same training and testing observation: \u2022 loan_amnt, funded_amnt_inv, term, int_rate, installment, grade, emp_length, home_ownership, annual_inc, verification_status, loan_status, purpose, title, dti, total_pymnt, delinq_2yrs, open_acc, pub_rec, last_pymnt_d, last_pymnt_amnt, application_type, revol_bal, revol_util, recoveries. The \u2018home_ownership\u2019 is a categorical variable with \u2018NA\u2019 values. Before proceeding, we need to filter out only the important values: \u2018Mortgage\u2019, \u2018Own\u2019 & \u2018Rent\u2019. # filtering only 3 values from the column lendingData <- lendingData %>% dplyr::filter(home_ownership %in% c(\"OWN\", \"RENT\", \"MORTGAGE\")) %>% droplevels() #levels(lendingData) <- c(\"MORTGAGE\", \"MORTGAGE\", \"MORTGAGE\", \"OWN\", \"RENT\") myL2DF <- lendingData %>% dplyr::select(\"loan_status\", \"annual_inc\", \"loan_amnt\",\"term\", \"funded_amnt_inv\", \"int_rate\", \"installment\", \"grade\", \"verification_status\", \"title\", \"dti\",\"total_pymnt\", \"delinq_2yrs\", \"last_pymnt_d\", \"last_pymnt_amnt\", \"application_type\",\"revol_bal\", \"revol_util\", \"emp_length\", \"pub_rec\", \"home_ownership\", \"recoveries\", \"open_acc\", \"pub_rec\", \"purpose\", \"title\", \"verification_status\") %>% dplyr:: filter(loan_status %in% c(\"Fully Paid\", \"Charged Off\")) %>% droplevels() table(lendingData$home_ownership) apply(is.na(myL2DF),2,sum) Dividing the data frame \u201cmyL2DF\u201d into two sets: Training & test, where training set will have 20% of the rows and test set will have 80% of the rows for analysis. Before moving on, we need to check for NULLs. All the columns with NULL values have to be ignored. trainRows2 <- createDataPartition(myL2DF$loan_status, p = 0.2, list=FALSE) trainRows2 myL2DFtrain <- myL2DF[trainRows2,] myL2DFtest <- myL2DF[-trainRows2,] # Imputing median in place of NULLs myL2DFtrain$last_pymnt_d[is.na(myL2DFtrain$last_pymnt_d)] <- median(myL2DFtrain$last_pymnt_d, na.rm=TRUE) myL2DFtrain$revol_util[is.na(myL2DFtrain$revol_util)] <- median(myL2DFtrain$revol_util, na.rm=TRUE) myL2DFtrain$dti[is.na(myL2DFtrain$dti)] <- median(myL2DFtrain$dti, na.rm=TRUE) myL2DFtest$last_pymnt_d[is.na(myL2DFtest$last_pymnt_d)] <- median(myL2DFtrain$last_pymnt_d, na.rm=TRUE) myL2DFtest$revol_util[is.na(myL2DFtest$revol_util)] <- median(myL2DFtrain$revol_util, na.rm=TRUE) myL2DFtest$dti[is.na(myL2DFtest$dti)] <- median(myL2DFtrain$dti, na.rm=TRUE) Since, we need to find the final loan status and for logistic regression we need two significant output values: \u201cFully Paid\u201d & \u201cCharged Off\u201d. Then we can check for the total number of \u201cFully Charged\u201d & \u201cFully Paid\u201d and assigning weights accordingly; sum(myL2DFtrain$loan_status == \"Fully Paid\") ## 8094 sum(myL2DFtrain$loan_status == \"Charged Off\") ## 2575 sum(myL2DFtrain$loan_status == \"Fully Paid\") sum(myL2DFtrain$loan_status == \"Charged Off\") myLendWeights2 <- numeric(nrow(myL2DFtrain)) myLendWeights2[myL2DFtrain$loan_status == \"Fully Paid\"] <- 1 myLendWeights2[myL2DFtrain$loan_status == \"Charged Off\"] <- 3","title":"Model-2 Classification trees"},{"location":"LCDA_Part3/#model-3-logistic-regression","text":"Running logistic regression using \u2018glm\u2019 function on the training set and summarizing the output. Then using the predict function on the test set for classifications based on the responses. Splitting these classes into two groups for the confusion matrix: \u201c< 0.5 as Charged Off & >= 0.5 as Fully Paid\u201d. myLendLR2 <- glm(loan_status ~ ., data=myL2DFtrain, weights=myLendWeights2, family=binomial(\"logit\")) summary(myLendLR2) # all probabilities as logistic regression myLendPredict2 <- predict(myLendLR2, newdata=myL2DFtest, type=\"response\") summary(myLendPredict2) # confusion matrix from test data myLendPredictLR2 <- character(length(myLendPredict2)) myLendPredictLR2[myLendPredict2 < 0.5] <- \"Charged Off\" myLendPredictLR2[myLendPredict2 >= 0.5] <- \"Fully Paid\" myLendLR2CM <- table(myL2DFtest$loan_status, myLendPredictLR2) myLendLR2CM 1-sum(diag(myLendLR2CM))/sum(myLendLR2CM) summary(myLendLR2) exp(coef(myLendLR2)) myLendPredictLR2 Charged Off Fully Paid Charged Off 10238 62 Fully Paid 16 32357 Misclassification rate: 0.001827854","title":"Model-3 Logistic Regression"},{"location":"LCDA_Part3/#model-4-classification-trees","text":"lendrpart2 <- rpart(loan_status ~ ., data=myL2DFtrain, weights = myLendWeights2) head(lendrpart2) #Plot the model plot(lendrpart2, uniform=TRUE,margin=0.01, asp = 0) text(lendrpart2, cex=.6) # Confusion matrix myPredictrpart2 <- predict(lendrpart2, newdata=myL2DFtest, type=\"class\") myPredictrpart2 myLendrpart2CM <- table(myL2DFtest$loan_status, myPredictrpart2) myLendrpart2CM 1-sum(diag(myLendrpart2CM))/sum(myLendrpart2CM) # misclassification rate myPredictrpart2 Charged Off Fully Paid Charged Off 9874 426 Fully Paid 1605 30768 Misclassification rate: 0.0475945 # ROC curve for logistic myLRPredict2 <- predict(myLendLR2, myL2DFtest, type=\"response\") myLRPred2 <- prediction(myLRPredict2, myL2DFtest$loan_status, label.ordering=c(\"Charged Off\", \"Fully Paid\")) myLRPerf2 <- performance(myLRPred2, \"tpr\", \"fpr\") # ROC curve for classification myrpartPredict2 <- predict(lendrpart2, myL2DFtest, type=\"prob\") # Only need positive for the plot hence '2' myrpartPred2 <- prediction(myrpartPredict2[,2], myL2DFtest$loan_status, label.ordering=c(\"Charged Off\", \"Fully Paid\")) myrpartPerf2 <- performance(myrpartPred2, \"tpr\", \"fpr\")","title":"Model-4 Classification trees"},{"location":"LCDA_Part3/#roc-curves","text":"plot(myrpartPerf2, col=18) plot(myLRPerf2, col=19, add=TRUE) plot(myrpartPerf, col=20, add=TRUE) plot(myLRPerf, col=21, add=TRUE) legend(0.5, 0.6, c(\"Log. Reg.1\", \"Class. Tree1\", \"Log. Reg.2\", \"Class. Tree2\"), col=c(21, 20, 19,18), lwd=3)","title":"ROC curves"},{"location":"LCDA_Part3/#area-under-curve","text":"performance(myLRPred, \"auc\") performance(myrpartPred, \"auc\") performance(myLRPred2, \"auc\") performance(myrpartPred2, \"auc\") Area under the ROC curve for model 1(Logistic regression): 0.6486718 Area under the ROC curve for model 2(Classification): 0.7584767 The curve for classification tree (model 2) is slightly higher than model 1, indicating a better performance across the different choices for false positive rate. At a false positive rate of 0.4 (so the true negative rate is 0.6), the true positive rate is about 0.8. Area under the ROC curve for model 3(Logistic regression): 0.9987902 Area under the ROC curve for model 4(Classification): 0.9890251 The curve for logistic regression (model 3) is slightly higher than model 4 and much higher than models 1 & 2, indicating good performance across the different choices for false positive rate. At a false positive rate of 0 (so the true negative rate is 1), the true positive rate is 1. It might seem that model 3 is the best model since it has the highest area under the curve and the lowest misclassification rate. However, using this loan performance data (that consists of variables considered during loan approval) will result in our model performing better with data than in actual practice because these variables are not known at the time of investment. The differences in performance between the models in Q #2 and Q #3 can be attributed to data leakage . It happens when information from outside the training dataset is used to create the model. Data leakage can lead to the creation overly optimistic predictive models. Leakage can be detected if we get performance that seems a little too good to be true like in model 3. The best model in my opinion is model 2 (Classification tree - dark blue line). It has AUC: 0.7584767.","title":"Area Under Curve"},{"location":"Read_LClub2017/","text":"```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) ## R Markdown ```{r} # reading the lendingdataclub 2017 Q3 .csv file # Scanning the file for reading all columns by skipping the first line since it is not required. colNames <- scan(\"LoanStats2017Q3.csv\", what=\"character\", skip=1, nlines=1, sep=\",\") # Specifying the character and factor columns for categorical data, # since rest of it is mostly numerical and date data. characterColumns <- c(\"id\", \"member_id\", \"emp_title\", \"issue_d\", \"url\", \"desc\", \"zip_code\", \"addr_state\", \"earliest_cr_line\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\", \"sec_app_earliest_cr_line\", \"hardship_type\", \"hardship_reason\", \"hardship_start_date\", \"hardship_end_date\", \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\") factorColumns <- c(\"term\", \"grade\", \"sub_grade\", \"emp_length\", \"home_ownership\", \"verification_status\", \"loan_status\", \"pymnt_plan\", \"purpose\", \"title\", \"initial_list_status\", \"policy_code\", \"application_type\", \"verification_status_joint\", \"hardship_flag\", \"hardship_status\", \"hardship_loan_status\", \"disbursement_method\", \"debt_settlement_flag\", \"settlement_status\") # Now reading the data and skipping the first line which is not required. # Excluding the top two rows and any other row that is not part of the main data by specifying # the exact number of rows (122701) lendingRows <- 122701 lendingData <- read.table(\"LoanStats2017Q3.csv\", skip=1, sep=\",\", nrows=lendingRows, colClasses = myColClasses, header=TRUE) # After reading the columns, two columns: revol_util and int_rate have % signs. # We don\u2019t want R to interpret it as character strings. Hence, we can convert to # character strings and getting rid of the \u2018%\u2019 and then back to numeric. lendingData$revol_util <- as.character(lendingData$revol_util) lendingData$revol_util <- sub(\"%\", \"\", lendingData$revol_util) lendingData$revol_util <- as.numeric(lendingData$revol_util) lendingData$int_rate <- as.character(lendingData$int_rate) lendingData$int_rate <- sub(\"%\", \"\", lendingData$int_rate) lendingData$int_rate <- as.numeric(lendingData$int_rate) # Formatting the columns for date data and specifying the first of the month since # the data only has month and year. dateColumns <- c(\"issue_d\", \"last_pymnt_d\", \"next_pymnt_d\", \"last_credit_pull_d\", \"sec_app_earliest_cr_line\", \"hardship_start_date\", \"hardship_end_date\", \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\", \"earliest_cr_line\") # Using function strptime(x, format), where x is a character vector of dates and format is a # character string of the dates, using percent symbols with characters to specify what types # of date and time information. # %d: Day of the month as decimal number (01--31) # %b: Abbreviated month name in the current locale on this platform # %Y: Year with century e.g.: 2015 lendingData[,dateColumns] <- apply(lendingData[,dateColumns], 2, function(x) { strptime(paste(\"1\", x), \"%d %b-%Y\")}) lendingData[,dateColumns] <- apply(lendingData[,dateColumns], 2, function(x) { strptime(paste(\"1\", x), \"%d %b-%Y\")}) # Saving the file as R object for faster loading. Then counting the number of each loan status. save(lendingData, file=\"LoanStats2017Q3.rda\") load(\"LoanStats2017Q3.rda\") (table(lendingData$loan_status)) # Charged Off Current Default Fully Paid In Grace Period # 12876 65816 161 40471 752 # Late (16-30 days) Late (31-120 days) # 503 2122","title":"Read_LClub2017"}]}